# AWS EKS VPC CNI Network Policy Simulator

## Overview

For anyone curious on how Network Policies are implemented in AWS EKS using the AWS VPC CNI plugin, this repo gives a 'stripped down' version of some of the components involved - no kubernetes required, just the minimum to have the same bpf programs running on a node taking the final decision to ALLOW or DROP a packet.

![full screen](https://github.com/user-attachments/assets/a8477da2-3753-43ae-b96e-10732393bb6c)

The main flow at a very high level:
- an ebpf program is attached to to the [ingress](ebpf/c/tc.v4ingress.bpf.c) / [egress](ebpf/c/tc.v4egress.bpf.c) path on a host veth tc extension point
- _pod_ ip addresses (network namespaced containers) and other metadata are dynamically published by the user space __aws-network-policy-agent__ to ebpf maps shared with the ebpf programs
- the ebpf programs would ALLOW, DENY, PASS the packet depending on value of those ebpf maps shared with user space

For a deep dive on how this is implemented, see a [good enough overview](NETWORK_POLICY_EXPLAINED.md) generated by Claude Opus. Likely not all the details are right, but gives enough info to understand the flow and when ultimately the bpf programs are invoked.

Why this project? To learn and share how leveraging some simple linux primitives (defining _epbf_ programs on the linux _netdev_ _tc_ extension points) can yield advanced functionalities.
One important lesson is the latency introduced by ebpf-based network policies evaluation to either ALLOW or DENY a packet: some basic metrics capturing gives us low digit microseconds:

![performance](https://github.com/user-attachments/assets/5bac998d-4431-42cb-a0ea-3356ccaf21e9)

You can also see where the program is executed in the linux kernel stack:

![stack-trace](https://github.com/user-attachments/assets/46a7c4dc-3077-4ccf-8412-efc409f1ec31)



Demo:

https://github.com/user-attachments/assets/a1c46a02-b401-433e-a4c3-924d7bde2dd0


## Quick Start

### Prerequisites

- ⚠️  needs to be running as root
    - installs libraries
    - creates network resources - bridges, veths etc
    - installs bpf probes  
- ⚠️  please run it in a vm (tested on Ubuntu 25:10)




```bash
# Must run as root
sudo ./setup.sh
```

### Launch TUI

Must run as root

```bash
sudo ./run.sh
```


## How It Works

To understand how it works - look at the video for a general idea and press "Save" from the output console.

<details>
  <summary><strong>Setup output (click to expand)</strong></summary>

  ```bash

WS EKS Network Policy Agent Simulator
Press 's' to run Setup, or use the buttons below

 Running as root


=== Running Multi-Pod Setup ===
Scenario: Backend service with ingress policy enforcement

Step 1: Environment Check
Objective: Verify root access and required tools
$ whoami
$ which clang bpftool tc ip

✓ Running as root
✓ clang found
✓ bpftool found
✓ tc found
✓ ip found
✓ ASM headers configured

 Validation: All required tools present

Step 2: Multi-Pod Network
Objective: Create 3-pod network (backend, allowed-client, denied-client)

Cleaning up old network configuration...
$ ip link del veth-be-h
$ ip netns del ns-backend
$ ip link del veth-allow-h
$ ip netns del ns-allowed
$ ip link del veth-deny-h
$ ip netns del ns-denied
$ ip link del br-sim

Creating bridge...
$ ip link add br-sim type bridge
$ ip addr add 10.0.0.1/24 dev br-sim
$ ip link set br-sim up

Setting up backend...
$ ip link add veth-be-h type veth peer name veth-be-p
$ ip link set veth-be-h master br-sim
$ ip link set veth-be-h up
$ ip netns add ns-backend
$ ip link set veth-be-p netns ns-backend
$ ip netns exec ns-backend ip addr add 10.0.0.10/24 dev veth-be-p
$ ip netns exec ns-backend ip route add default via 10.0.0.1

Setting up allowed-client...
$ ip link add veth-allow-h type veth peer name veth-allow-p
$ ip link set veth-allow-h master br-sim
$ ip link set veth-allow-h up
$ ip netns add ns-allowed
$ ip link set veth-allow-p netns ns-allowed
$ ip netns exec ns-allowed ip addr add 10.0.0.20/24 dev veth-allow-p
$ ip netns exec ns-allowed ip route add default via 10.0.0.1

Setting up denied-client...
$ ip link add veth-deny-h type veth peer name veth-deny-p
$ ip link set veth-deny-h master br-sim
$ ip link set veth-deny-h up
$ ip netns add ns-denied
$ ip link set veth-deny-p netns ns-denied
$ ip netns exec ns-denied ip addr add 10.0.0.30/24 dev veth-deny-p
$ ip netns exec ns-denied ip route add default via 10.0.0.1


 Validation: Multi-pod network configured
Network Summary:
  Bridge: br-sim (10.0.0.1/24)
  Backend: 10.0.0.10 (ns-backend, veth-be-h/p, port 8080)
  Allowed: 10.0.0.20 (ns-allowed, veth-al-h/p)
  Denied: 10.0.0.30 (ns-denied, veth-de-h/p)

Step 3: Compile AWS VPC CNI BPF
Objective: Compile AWS VPC CNI eBPF program
Source: ebpf/c/tc.v4ingress.bpf.c
$ clang -O2 -target bpf -c tc.v4ingress.bpf.c -o tc.v4ingress.bpf.o
Compiling ingress program...
$ clang -O2 -target bpf -c tc.v4ingress.bpf.c -o tc.v4ingress.bpf.o
Created: tc.v4ingress.bpf.o

Compiling egress program...
$ clang -O2 -target bpf -c tc.v4egress.bpf.c -o tc.v4egress.bpf.o
Created: tc.v4egress.bpf.o

 Validation: Compilation successful - tc.v4ingress.bpf.o created

Step 4: Load BPF to Backend
Objective: Load and attach ingress & egress eBPF programs

Setting up TC qdisc...
$ tc qdisc add dev veth-be-h clsact

Cleaning old BPF filters...
$ tc filter del dev veth-be-h egress
$ tc filter del dev veth-be-h ingress

Cleaning pinned BPF maps...
$ rm -f /sys/fs/bpf/ingress_map
$ rm -f /sys/fs/bpf/cp_ingress_map
$ rm -f /sys/fs/bpf/ingress_pod_state_map
$ rm -f /sys/fs/bpf/aws_conntrack_map
$ rm -f /sys/fs/bpf/stack_traces
$ rm -f /sys/fs/bpf/policy_events
$ rm -f kprobe_stacktrace
$ rm -f kprobe_stack_link

Loading ingress BPF program...
$ tc filter add dev veth-be-h egress bpf da obj tc.v4ingress.bpf.o sec tc_cls
BPF program attached to veth-be-h

Initializing pod state maps...
$ bpftool map update id 17 key 0x00000000 value 0x00
$ bpftool map update id 17 key 0x01000000 value 0x00
Ingress pod state: POLICIES_APPLIED (deny-by-default)
$ bpftool map update id 22 key 0x00000000 value 0x01
$ bpftool map update id 22 key 0x01000000 value 0x01
Egress pod state: DEFAULT_ALLOW (permit all outbound)
Pod state maps initialized

Program ID: 245
$ bpftool map pin id 78 /sys/fs/bpf/policy_events
Ring buffer pinned for event streaming
Loading egress BPF program...
$ tc filter add dev veth-be-h ingress bpf da obj tc.v4egress.bpf.o sec tc_cls
BPF program attached to veth-be-h

Program ID: 250
Egress program loaded successfully

 Validation: Program loaded on veth-be-h - ID 245
  Egress Program ID: 250
  Map ID: 21

═══ BPF Performance Monitoring ═══
Starting bpftop-style performance monitor...
 BPF stats monitoring started
  Live stats shown in the performance panel above

═══ Stack Trace Capture (cls_bpf_classify kprobe) ═══
Attaching kprobe to capture stacks at TC BPF decision point...
 Failed to load stack trace kprobe
  BCC may not be installed or cls_bpf_classify not available

Step 5: Start Backend Server
Objective: Start TCP server on backend pod port 8080
$ ip netns exec ns-backend nc -l -k -p 8080 &
 Validation: Backend server started on 10.0.0.10:8080

Step 6: Configure Network Policies
Objective: Allow allowed-client, enforce deny-by-default
Found map 'ingress_pod_state_map' with ID: 17 (from program 245)
$ bpftool map update id 21 key 10.0.0.20/32
$ bpftool map update id 17 key 0x00000000 value 0x00
 Validation: 10.0.0.20 added to ingress_map (allowed)
Found map 'ingress_pod_state_map' with ID: 17 (from program 245)
 Policy enforcement enabled - deny-by-default active


=== Setup Complete ===
Network configured with policies:
  - Allowed client (10.0.0.20) can connect
  - Denied client (10.0.0.30) is blocked
Ready to test:
  1. Click 'Test Allowed' (or press 'a') - should succeed
  2. Click 'Test Denied' (or press 'd') - should fail (blocked)


=== System Status Report ===

Step 0: Environment Check ✅ PASS
  ✓ Running as root
  ✓ clang available
  ✓ bpftool available
  ✓ tc available
  ✓ ip available
  ✓ ASM headers configured

Step 1: Network Setup ✅ PASS
  ✓ Namespace ns-backend exists
  ✓ Namespace ns-allowed exists
  ✓ Namespace ns-denied exists
  ✓ Interface veth-be-h exists

Step 2: Program Compilation ✅ PASS
  ✓ Ingress program compiled (38696 bytes)
  ✓ Egress program compiled (39064 bytes)

Step 3: TC Qdisc Setup ✅ PASS
  ✓ clsact qdisc attached to veth-be-h

Step 4: BPF Program Load ✅ PASS
  ✓ Ingress program attached (TC egress)
    Program ID: 245
  ✓ Egress program attached (TC ingress)
    Program ID: 250

Step 5: BPF Map Configuration ✅ PASS
  ✓ ingress_map found (ID: 21)
    Allowed IPs: 1
  ✓ ingress_pod_state_map found (ID: 17)
    Policy mode: POLICIES_APPLIED (deny-by-default)
  ✓ aws_conntrack_map found (ID: 19)

Step 6: Connectivity Test ✅ PASS
  ✓ Allowed client (10.0.0.20) can connect
  ✓ Denied client (10.0.0.30) BLOCKED (timeout)

✅ Overall Status: PASS


=== Setup Complete ===
Network configured with policies:
  - Allowed client (10.0.0.20) can connect
  - Denied client (10.0.0.30) is blocked
Ready to test:
  1. Click 'Test Allowed' (or press 'a') - should succeed
  2. Click 'Test Denied' (or press 'd') - should fail (blocked)

Test: Allowed client (10.0.0.20) → Backend (10.0.0.10:8080)
$ ip netns exec ns-allowed timeout 2 nc -zv 10.0.0.10 8080
 Allowed client can connect (expected)

Test: Denied client (10.0.0.30) → Backend (10.0.0.10:8080)
$ ip netns exec ns-denied timeout 2 nc -zv 10.0.0.10 8080
 Denied client blocked (expected)
```
</details>

## License

- AWS VPC CNI source: Apache 2.0
- Modifications: Same (Apache 2.0)

## References

- [AWS Network Policy Agent](https://github.com/aws/aws-network-policy-agent)
- [eBPF TC Documentation](https://docs.kernel.org/bpf/prog_cgroup_sockopt.html)
- [BPF Type Format (BTF)](https://www.kernel.org/doc/html/latest/bpf/btf.html)
